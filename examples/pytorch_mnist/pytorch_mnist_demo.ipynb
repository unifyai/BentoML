{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a682ea0b",
   "metadata": {},
   "source": [
    "# BentoML PyTorch MNIST Tutorial\n",
    "\n",
    "Link to source code: https://github.com/bentoml/BentoML/tree/main/examples/pytorch_mnist/\n",
    "\n",
    "Install required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad00863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45393b74",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "First let's define a simple PyTorch network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caeff07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Convolutional Neural Network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(26 * 26 * 10, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def predict(self, inp):\n",
    "        \"\"\"predict digit for input\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            raw_output = self(inp)\n",
    "            _, pred = torch.max(raw_output, 1)\n",
    "            return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38888f0a",
   "metadata": {},
   "source": [
    "## Training and Saving the model\n",
    "\n",
    "Then we define a simple PyTorch network and some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62db15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import bentoml\n",
    "\n",
    "# reproducible setup for testing\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def _dataloader_init_fn(worker_id):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539b5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_FOLDS = 5\n",
    "NUM_EPOCHS = 5\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    # Prepare MNIST dataset by concatenating Train/Test part; we split later.\n",
    "    train_set = MNIST(\n",
    "        os.getcwd(), download=True, transform=transforms.ToTensor(), train=True\n",
    "    )\n",
    "    test_set = MNIST(\n",
    "        os.getcwd(), download=True, transform=transforms.ToTensor(), train=False\n",
    "    )\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, loss_function, train_loader, epoch, device=\"cpu\"):\n",
    "    # Mark training flag\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 499 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(inputs),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device=\"cpu\"):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2db4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_set, test_set = get_dataset()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=10,\n",
    "    sampler=torch.utils.data.RandomSampler(test_set),\n",
    "    worker_init_fn=_dataloader_init_fn,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "788c19a0",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "We can do some cross validation and the results can be saved with the model as metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2fdd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(dataset, epochs=NUM_EPOCHS, k_folds=K_FOLDS, device='cpu'):\n",
    "    results = {}\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "        print(f\"FOLD {fold}\")\n",
    "        print(\"--------------------------------\")\n",
    "\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=10,\n",
    "            sampler=train_subsampler,\n",
    "            worker_init_fn=_dataloader_init_fn,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=10,\n",
    "            sampler=test_subsampler,\n",
    "            worker_init_fn=_dataloader_init_fn,\n",
    "        )\n",
    "\n",
    "        # Train this fold\n",
    "        model = SimpleConvNet().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            train_epoch(model, optimizer, loss_function, train_loader, epoch, device)\n",
    "\n",
    "        # Evaluation for this fold\n",
    "        correct, total = test_model(model, test_loader, device)\n",
    "        print(\"Accuracy for fold %d: %d %%\" % (fold, 100.0 * correct / total))\n",
    "        print(\"--------------------------------\")\n",
    "        results[fold] = 100.0 * (correct / total)\n",
    "\n",
    "    # Print fold results\n",
    "    print(f\"K-FOLD CROSS VALIDATION RESULTS FOR {K_FOLDS} FOLDS\")\n",
    "    print(\"--------------------------------\")\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f\"Fold {key}: {value} %\")\n",
    "        sum += value\n",
    "\n",
    "    print(f\"Average: {sum/len(results.items())} %\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd06de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.273127\n",
      "Train Epoch: 0 [4990/60000 (10%)]\tLoss: 1.362216\n",
      "Train Epoch: 0 [9980/60000 (21%)]\tLoss: 0.411742\n",
      "Train Epoch: 0 [14970/60000 (31%)]\tLoss: 1.001366\n",
      "Train Epoch: 0 [19960/60000 (42%)]\tLoss: 0.102839\n",
      "Train Epoch: 0 [24950/60000 (52%)]\tLoss: 0.510927\n",
      "Train Epoch: 0 [29940/60000 (62%)]\tLoss: 0.053766\n",
      "Train Epoch: 0 [34930/60000 (73%)]\tLoss: 0.231184\n",
      "Train Epoch: 0 [39920/60000 (83%)]\tLoss: 0.372940\n",
      "Train Epoch: 0 [44910/60000 (94%)]\tLoss: 0.128234\n",
      "Accuracy for fold 0: 91 %\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.327491\n",
      "Train Epoch: 0 [4990/60000 (10%)]\tLoss: 1.318107\n",
      "Train Epoch: 0 [9980/60000 (21%)]\tLoss: 0.485991\n",
      "Train Epoch: 0 [14970/60000 (31%)]\tLoss: 1.216311\n",
      "Train Epoch: 0 [19960/60000 (42%)]\tLoss: 0.419982\n",
      "Train Epoch: 0 [24950/60000 (52%)]\tLoss: 0.758951\n",
      "Train Epoch: 0 [29940/60000 (62%)]\tLoss: 0.147900\n",
      "Train Epoch: 0 [34930/60000 (73%)]\tLoss: 0.171809\n",
      "Train Epoch: 0 [39920/60000 (83%)]\tLoss: 0.317513\n",
      "Train Epoch: 0 [44910/60000 (94%)]\tLoss: 0.669927\n",
      "Accuracy for fold 1: 91 %\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.218862\n",
      "Train Epoch: 0 [4990/60000 (10%)]\tLoss: 1.440863\n",
      "Train Epoch: 0 [9980/60000 (21%)]\tLoss: 0.390616\n",
      "Train Epoch: 0 [14970/60000 (31%)]\tLoss: 0.372521\n",
      "Train Epoch: 0 [19960/60000 (42%)]\tLoss: 0.524743\n",
      "Train Epoch: 0 [24950/60000 (52%)]\tLoss: 0.206889\n",
      "Train Epoch: 0 [29940/60000 (62%)]\tLoss: 0.041551\n",
      "Train Epoch: 0 [34930/60000 (73%)]\tLoss: 0.222209\n",
      "Train Epoch: 0 [39920/60000 (83%)]\tLoss: 0.570893\n",
      "Train Epoch: 0 [44910/60000 (94%)]\tLoss: 0.221007\n",
      "Accuracy for fold 2: 90 %\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.316931\n",
      "Train Epoch: 0 [4990/60000 (10%)]\tLoss: 0.965538\n",
      "Train Epoch: 0 [9980/60000 (21%)]\tLoss: 0.240634\n",
      "Train Epoch: 0 [14970/60000 (31%)]\tLoss: 0.375480\n",
      "Train Epoch: 0 [19960/60000 (42%)]\tLoss: 0.150217\n",
      "Train Epoch: 0 [24950/60000 (52%)]\tLoss: 0.608829\n",
      "Train Epoch: 0 [29940/60000 (62%)]\tLoss: 0.035714\n",
      "Train Epoch: 0 [34930/60000 (73%)]\tLoss: 0.420194\n",
      "Train Epoch: 0 [39920/60000 (83%)]\tLoss: 0.175810\n",
      "Train Epoch: 0 [44910/60000 (94%)]\tLoss: 0.158451\n",
      "Accuracy for fold 3: 92 %\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.352834\n",
      "Train Epoch: 0 [4990/60000 (10%)]\tLoss: 0.953534\n",
      "Train Epoch: 0 [9980/60000 (21%)]\tLoss: 0.918018\n",
      "Train Epoch: 0 [14970/60000 (31%)]\tLoss: 0.854282\n",
      "Train Epoch: 0 [19960/60000 (42%)]\tLoss: 0.248393\n",
      "Train Epoch: 0 [24950/60000 (52%)]\tLoss: 0.237291\n",
      "Train Epoch: 0 [29940/60000 (62%)]\tLoss: 0.449530\n",
      "Train Epoch: 0 [34930/60000 (73%)]\tLoss: 0.225519\n",
      "Train Epoch: 0 [39920/60000 (83%)]\tLoss: 0.256260\n",
      "Train Epoch: 0 [44910/60000 (94%)]\tLoss: 0.180360\n",
      "Accuracy for fold 4: 90 %\n",
      "--------------------------------\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 91.13333333333333 %\n",
      "Fold 1: 91.3 %\n",
      "Fold 2: 90.81666666666666 %\n",
      "Fold 3: 92.07499999999999 %\n",
      "Fold 4: 90.9 %\n",
      "Average: 91.245 %\n"
     ]
    }
   ],
   "source": [
    "cv_results = cross_validate(train_set, epochs=1, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad2104a6",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d311c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs=NUM_EPOCHS, device=\"cpu\"):\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=10,\n",
    "        sampler=train_sampler,\n",
    "        worker_init_fn=_dataloader_init_fn,\n",
    "    )\n",
    "    model = SimpleConvNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        train_epoch(model, optimizer, loss_function, train_loader, epoch, device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8df05c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.371420\n",
      "Train Epoch: 0 [4990/60000 (8%)]\tLoss: 1.000866\n",
      "Train Epoch: 0 [9980/60000 (17%)]\tLoss: 1.110684\n",
      "Train Epoch: 0 [14970/60000 (25%)]\tLoss: 0.737674\n",
      "Train Epoch: 0 [19960/60000 (33%)]\tLoss: 0.364441\n",
      "Train Epoch: 0 [24950/60000 (42%)]\tLoss: 0.390488\n",
      "Train Epoch: 0 [29940/60000 (50%)]\tLoss: 0.282015\n",
      "Train Epoch: 0 [34930/60000 (58%)]\tLoss: 0.199639\n",
      "Train Epoch: 0 [39920/60000 (67%)]\tLoss: 0.367602\n",
      "Train Epoch: 0 [44910/60000 (75%)]\tLoss: 0.554657\n",
      "Train Epoch: 0 [49900/60000 (83%)]\tLoss: 0.017670\n",
      "Train Epoch: 0 [54890/60000 (91%)]\tLoss: 0.086635\n",
      "Train Epoch: 0 [59880/60000 (100%)]\tLoss: 0.249397\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.222882\n",
      "Train Epoch: 1 [4990/60000 (8%)]\tLoss: 0.287983\n",
      "Train Epoch: 1 [9980/60000 (17%)]\tLoss: 0.151059\n",
      "Train Epoch: 1 [14970/60000 (25%)]\tLoss: 0.118343\n",
      "Train Epoch: 1 [19960/60000 (33%)]\tLoss: 0.756619\n",
      "Train Epoch: 1 [24950/60000 (42%)]\tLoss: 0.100837\n",
      "Train Epoch: 1 [29940/60000 (50%)]\tLoss: 0.133997\n",
      "Train Epoch: 1 [34930/60000 (58%)]\tLoss: 1.145330\n",
      "Train Epoch: 1 [39920/60000 (67%)]\tLoss: 0.029823\n",
      "Train Epoch: 1 [44910/60000 (75%)]\tLoss: 0.175509\n",
      "Train Epoch: 1 [49900/60000 (83%)]\tLoss: 0.097101\n",
      "Train Epoch: 1 [54890/60000 (91%)]\tLoss: 0.556650\n",
      "Train Epoch: 1 [59880/60000 (100%)]\tLoss: 0.014780\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.117354\n",
      "Train Epoch: 2 [4990/60000 (8%)]\tLoss: 0.010053\n",
      "Train Epoch: 2 [9980/60000 (17%)]\tLoss: 0.034273\n",
      "Train Epoch: 2 [14970/60000 (25%)]\tLoss: 0.112337\n",
      "Train Epoch: 2 [19960/60000 (33%)]\tLoss: 0.126786\n",
      "Train Epoch: 2 [24950/60000 (42%)]\tLoss: 0.101872\n",
      "Train Epoch: 2 [29940/60000 (50%)]\tLoss: 0.008624\n",
      "Train Epoch: 2 [34930/60000 (58%)]\tLoss: 0.267356\n",
      "Train Epoch: 2 [39920/60000 (67%)]\tLoss: 0.122472\n",
      "Train Epoch: 2 [44910/60000 (75%)]\tLoss: 0.114122\n",
      "Train Epoch: 2 [49900/60000 (83%)]\tLoss: 0.117053\n",
      "Train Epoch: 2 [54890/60000 (91%)]\tLoss: 0.054726\n",
      "Train Epoch: 2 [59880/60000 (100%)]\tLoss: 0.672942\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.139965\n",
      "Train Epoch: 3 [4990/60000 (8%)]\tLoss: 0.257957\n",
      "Train Epoch: 3 [9980/60000 (17%)]\tLoss: 0.064906\n",
      "Train Epoch: 3 [14970/60000 (25%)]\tLoss: 0.057580\n",
      "Train Epoch: 3 [19960/60000 (33%)]\tLoss: 0.020287\n",
      "Train Epoch: 3 [24950/60000 (42%)]\tLoss: 0.024309\n",
      "Train Epoch: 3 [29940/60000 (50%)]\tLoss: 0.113165\n",
      "Train Epoch: 3 [34930/60000 (58%)]\tLoss: 0.096085\n",
      "Train Epoch: 3 [39920/60000 (67%)]\tLoss: 0.281361\n",
      "Train Epoch: 3 [44910/60000 (75%)]\tLoss: 0.016093\n",
      "Train Epoch: 3 [49900/60000 (83%)]\tLoss: 0.104659\n",
      "Train Epoch: 3 [54890/60000 (91%)]\tLoss: 0.042764\n",
      "Train Epoch: 3 [59880/60000 (100%)]\tLoss: 0.291086\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.489768\n",
      "Train Epoch: 4 [4990/60000 (8%)]\tLoss: 0.013543\n",
      "Train Epoch: 4 [9980/60000 (17%)]\tLoss: 0.093427\n",
      "Train Epoch: 4 [14970/60000 (25%)]\tLoss: 0.030668\n",
      "Train Epoch: 4 [19960/60000 (33%)]\tLoss: 0.051385\n",
      "Train Epoch: 4 [24950/60000 (42%)]\tLoss: 0.505770\n",
      "Train Epoch: 4 [29940/60000 (50%)]\tLoss: 0.141526\n",
      "Train Epoch: 4 [34930/60000 (58%)]\tLoss: 0.036125\n",
      "Train Epoch: 4 [39920/60000 (67%)]\tLoss: 0.298740\n",
      "Train Epoch: 4 [44910/60000 (75%)]\tLoss: 0.288054\n",
      "Train Epoch: 4 [49900/60000 (83%)]\tLoss: 0.178683\n",
      "Train Epoch: 4 [54890/60000 (91%)]\tLoss: 0.005956\n",
      "Train Epoch: 4 [59880/60000 (100%)]\tLoss: 0.012567\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(train_set, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92d9b23c",
   "metadata": {},
   "source": [
    "### saving the model with an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7943e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f52eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save an image tensor for eager transpile\n",
    "img = PIL.Image.open(\"samples/0.png\")\n",
    "np_img = np.array(img)\n",
    "tensor_img = torch.from_numpy(np_img).float()\n",
    "tensor_img = tensor_img.unsqueeze(0).unsqueeze(0)\n",
    "tensor_img = torch.nn.functional.interpolate(tensor_img, size=28, mode='bicubic', align_corners=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fe9c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = test_model(trained_model, test_loader, device=device)\n",
    "metadata = {\n",
    "    \"accuracy\": float(correct) / total,\n",
    "    \"cv_stats\": cv_results,\n",
    "}\n",
    "\n",
    "tag = bentoml.pytorch.save_model(\n",
    "    \"pytorch_mnist\",\n",
    "    trained_model,\n",
    "    metadata=metadata,\n",
    "    custom_objects={'input': tensor_img}, # will allow eager transpile during load\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "271dd5e4",
   "metadata": {},
   "source": [
    "# Loading model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25fc082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import jax\n",
    "import random\n",
    "\n",
    "import bentoml\n",
    "\n",
    "# reproducible setup for testing\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493a81fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(tag=\"pytorch_mnist:oac3zvakswac5nfv\", path=\"/home/rishab/bentoml/models/pytorch_mnist/oac3zvakswac5nfv\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bentoml.models.list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f415df44",
   "metadata": {},
   "source": [
    "### load flax version\n",
    "after using ivy transpile magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa20d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\"\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "flax_graph, params = bentoml.flax.load_model(\"pytorch_mnist:latest\", ivy_transpile=True, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d69a71d6",
   "metadata": {},
   "source": [
    "### load normal pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33fd1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = bentoml.pytorch.load_model(\"pytorch_mnist:latest\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7da5426c",
   "metadata": {},
   "source": [
    "### compare outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bf9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(\"samples/0.png\")\n",
    "np_img = np.array(img)\n",
    "tensor_img = torch.from_numpy(np_img).float()\n",
    "tensor_img = tensor_img.unsqueeze(0).unsqueeze(0)\n",
    "tensor_img = torch.nn.functional.interpolate(tensor_img, size=28, mode='bicubic', align_corners=False).to(device)\n",
    "\n",
    "np_image = tensor_img.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f600ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def infer(np_image):\n",
    "    return flax_graph.apply(params, np_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f88489",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model.to(device)\n",
    "\n",
    "def _f(args):\n",
    "  return torch_model(args)\n",
    "\n",
    "comp_model = torch.compile(_f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4743c72",
   "metadata": {},
   "source": [
    "### inference latency comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb027dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 µs ± 115 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = comp_model(tensor_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c163e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.4 µs ± 9.34 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = infer(np_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b506e1ee",
   "metadata": {},
   "source": [
    "### sanity check\n",
    "let’s load a different image and make sure that the results are the same in both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b73351c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open(\"samples/3.png\")\n",
    "np_img = np.array(img)\n",
    "tensor_img = torch.from_numpy(np_img).float()\n",
    "tensor_img = tensor_img.unsqueeze(0).unsqueeze(0)\n",
    "tensor_img = torch.nn.functional.interpolate(tensor_img, size=28, mode='bicubic', align_corners=False).to(device)\n",
    "np_image = tensor_img.detach().cpu().numpy()\n",
    "\n",
    "jax_outputs = infer(np_image)\n",
    "torch_output = comp_model(tensor_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c5b67c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(torch_output.detach().cpu().numpy(), jax_outputs, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d57952c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ True], dtype=bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch_output, 1)[1].item() == np.argmax(jax_outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9f916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "name": "pytorch_mnist.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
